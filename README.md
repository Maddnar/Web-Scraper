# Web-Scraper

## Objective
My primary objective when starting this project was to gain a basic understanding of what it entails to extract data from a website and store that data effectively. This was achieved through the use of the Python library Scrapy for data extraction, MongoDB for data storage and organization, and a controlled website serving as our target. 
<br/>
<a href="https://realpython.com/web-scraping-with-scrapy-and-mongodb/#prepare-the-scraper-scaffolding"> Link to Original Project Guide</a>



### Skills Learned

- Setting up and working with different Python virtual environments
- Ability to recognize CSS selectors to identify desired web content
- MongoDB integration for simplified data management
- Data pipeline design from extraction -> transformation -> storage
- Ethical data scraping practices like respecting robots.txt files

### Tools Used

- Scrapy for an efficient all-in-one data scraping framework
- MongoDB for a scalable and easy-to-work-with database for storing and querying scraped data.
- PyMongo as a way to interact with my MongoDB database from Python programs.

## Key Moments

Rather than recounting every step I took to complete this project, I will review some of the key learning moments I experienced throughout the process.
